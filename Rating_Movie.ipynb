{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzouttini/Exam-Deep-Learning/blob/main/Rating_Movie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prediction Rating Movie**\n",
        "\n",
        "In this project I am going to predict Movie's rating based on their reviews."
      ],
      "metadata": {
        "id": "wwwO3vHK_r-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 0: Introduction to the model\n",
        "The architecure I choose for this project is formed by a **RNN unit** (GRU or LSTM) followed by one (or more) **MLP layers**. <br>\n",
        "As introductory part of the code, I have to import all the libraries that will be necessary for the running of the model."
      ],
      "metadata": {
        "id": "AbutYtJt_37m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ntsfhSSL8Efa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries from Tensorflow and Keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n"
      ],
      "metadata": {
        "id": "mORTHm3cCMgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries useful for unzip of the file\n",
        "import os\n",
        "import requests\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "jlX9JemLB9W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries from Sklearn\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_curve, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split,  StratifiedKFold, GridSearchCV\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ],
      "metadata": {
        "id": "dHPVnRQyvk_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Special library from Scikeras (library that unite Keras and Scikitlearn)\n",
        "! pip install scikeras\n",
        "from scikeras.wrappers import KerasClassifier     #we are going to wrap keras with scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7pQOr7GulnO",
        "outputId": "910f43e0-f35f-491d-eaf7-852bc6aa873c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.0: Unzip and Read of the file\n",
        "Since on GitHub we cannot upload file that are greater than 25 mb, I decide to **zip** it and upload on GitHub. <br>\n",
        "Then I create a function to unzip and, consequently, read it as csv file."
      ],
      "metadata": {
        "id": "lChdTsy9CvNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip_and_read_csv(url, csv_filename):\n",
        "    # Download the zip file\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Save the zip file\n",
        "    zip_filename = 'temp.zip'\n",
        "    with open(zip_filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    # Read the CSV file into a pandas DataFrame\n",
        "    csv_filepath = os.path.join(os.getcwd(), csv_filename)\n",
        "    df = pd.read_csv(csv_filepath,encoding='latin-1')\n",
        "\n",
        "    # Clean up temporary files\n",
        "    os.remove(zip_filename)\n",
        "    os.remove(csv_filepath)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ScU-lbmOrps-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define my GitHub link and the name of our dataset (csv)\n",
        "url = \"https://github.com/lorenzouttini/Exam-Deep-Learning/raw/main/parkReviews.zip\"\n",
        "csv_filename = 'parkReviews.csv'"
      ],
      "metadata": {
        "id": "BzHoDt7sDoql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function and show the columns of the dataset\n",
        "df = unzip_and_read_csv(url, csv_filename)\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzwmtYuhnxS4",
        "outputId": "4959e7a8-a9a9-4ca4-f64e-c35d4b1daf2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Review_ID', 'Rating', 'Year_Month', 'Reviewer_Location', 'Review_Text',\n",
              "       'Branch'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 1: Input representation and Preprocessing of the data\n",
        "As I explained on the project, I decide to use as input only the *'Review_Text'* feature. <br>\n",
        "Since it is a text feature, I have to do some preprocessing on it before to pass as input of the RNN unit. <br>\n",
        "Firstly, I **reduce** the rows of the dataset. <br>\n",
        "Secondly, I have **tokenized** it. <br>\n",
        "Finally, I create the embedding (**one-hot encoding**)."
      ],
      "metadata": {
        "id": "A9AaTeI6EJjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1: Reduce the rows of the dataset\n",
        "Since the dataset has more than 42.000 rows and we have limited RAM, I decide to reduce the dataset of 90% and keep only reviews that are more than 20 words and less than 200 words."
      ],
      "metadata": {
        "id": "I5AuZyveFStN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove 90% of the rows\n",
        "df = df.sample(frac=0.08, random_state=42)"
      ],
      "metadata": {
        "id": "Qqq2QYpI8mh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove reviews with more than 200 words or less than 20 words\n",
        "df = df[df['Review_Text'].str.split().apply(len).between(20, 200)]\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB4_GszF8ohx",
        "outputId": "36f4b53c-98a4-44ad-b0b6-36b357c200a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2749, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2: Adjust the balance of the classes (added, not in the project)\n",
        "It is important that in a model the percentage of the classes in the output is almost equal. <br>\n",
        "Since in this dataset the classes are too much imbalanced, I have used the integrated *\"RandomUnderSampler\"* to reduce the percentage of the most likely classes and have a balanced dataset. <br>\n",
        "In this way the performances are **not influenced** by the percentage of some classes."
      ],
      "metadata": {
        "id": "8cJsyHjtGR7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call our dataset\n",
        "target_counts = df['Rating'].value_counts()\n",
        "\n",
        "# Compute the percentage of each rating\n",
        "target_percentage = (target_counts / len(df)) * 100\n",
        "\n",
        "print(target_percentage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egg76YNv_Xq5",
        "outputId": "1cf7fad1-3567-47e0-8c91-bb34e3e334aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5    56.929793\n",
            "4    25.245544\n",
            "3    10.985813\n",
            "2     3.928701\n",
            "1     2.910149\n",
            "Name: Rating, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into X (Rating review) and y (Rating stars)\n",
        "X = df['Review_Text']\n",
        "y = df['Rating']"
      ],
      "metadata": {
        "id": "9ER2Q21d8qAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of RandomUnderSampler\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "\n",
        "original_shape = X.shape\n",
        "\n",
        "# Reshape in a numpy array\n",
        "X = np.array(X).reshape(-1, 1)\n",
        "\n",
        "# Perform undersampling\n",
        "X, y = undersampler.fit_resample(X, y)\n",
        "\n",
        "# Create a new DataFrame with the undersampled data\n",
        "df = pd.DataFrame(X, columns=['Rating'])\n",
        "df['Rating'] = y\n",
        "\n",
        "# Shuffle the undersampled data\n",
        "df = df.sample(frac=1, random_state=42)"
      ],
      "metadata": {
        "id": "2c2nB2XamylE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the new dimension of X after UnderSampling\n",
        "new_shape = X.shape\n",
        "X.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVOKA9XarJ2U",
        "outputId": "f6e735dc-d895-4ac4-f0cf-c0e6bc360387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Return X to the original shape\n",
        "X = X.reshape(new_shape[0])\n",
        "\n",
        "# Convert X_undersampled to a pandas Series\n",
        "X = pd.Series(X.squeeze())"
      ],
      "metadata": {
        "id": "XUdETN7rqq54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that our dataset is **balanced**."
      ],
      "metadata": {
        "id": "O_aiQqHmIog7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_counts = df['Rating'].value_counts()\n",
        "\n",
        "# New percentage\n",
        "target_percentage = (target_counts / len(df)) * 100\n",
        "\n",
        "print(target_percentage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46G-pDdNpp1O",
        "outputId": "e95ce103-6bf1-489c-b336-da2f7b88a806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3    20.0\n",
            "4    20.0\n",
            "1    20.0\n",
            "2    20.0\n",
            "5    20.0\n",
            "Name: Rating, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decrease the rating of one unit (now it is from 0 to 4)\n",
        "df['Rating'] = df['Rating'] - 1"
      ],
      "metadata": {
        "id": "zwhWWiRxIPg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ-vNrQbwULD",
        "outputId": "75ceae66-eeaf-4633-d2a2-ac961a248838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3: Tokenize\n",
        "**Tokenization** of the text is another fundamental part in the preprocessing. <br>\n",
        "As I explained in the project, the tokenization part consists in making all the words lower case, removing sign punctuations and separate the words in a list using space. Moreover, I have defined the vocabulary. <br>\n",
        "For this type of task I have used a **pretrained Tokenizer** from Keras. <br>"
      ],
      "metadata": {
        "id": "MTv2dW5kJGfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vocabulary dictionary\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "0mDdHl749C1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4: One-Hot Encoding\n",
        "Since I have reduced the dataset to a small size, I am able to perform the embedding through one-hot encoding. <br>\n",
        "Also in this case, I decide to use a pretrained one-hot encoding from keras.  that tranforms all the sequences vectors of **2 dimensions**:\n",
        "- First: size of the dataset.\n",
        "- Second: lenght of the sequence. <br>\n",
        "\n",
        "Since the RNN unit accepts only inputs of the same shape, I have to pad all the sequences to a common dimension (padding). In this case I choose 150 and this is the second dimension of the one-hot vector."
      ],
      "metadata": {
        "id": "swvf2xyFK0jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform words into a two-dimensional one-hot encoding vectors\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=150, padding='post')\n",
        "padded_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoajvcAM9GRg",
        "outputId": "5de2fcaf-b0c3-4d31-bcee-51af2195272c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5: Train and Test\n",
        "In the last part of the input preprocessing, I divide X,y in train and test."
      ],
      "metadata": {
        "id": "uAXgq6yfMqGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "172SJiMO9Nev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the input data to 3D\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)"
      ],
      "metadata": {
        "id": "36DQwatY-ExX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 2: Output Layer\n",
        "\n",
        "As I have described in the project, the output is a **vector a probabilities**, each one assigned to a class.\n",
        "\n",
        "The output layer is Dense layer with **5 units** (where this number corresponds to the number of classes). <br>\n",
        "\n",
        "\n",
        "Since it is a multiclass task, the activation function I have used for this layer is the **softmax**. <br>"
      ],
      "metadata": {
        "id": "wkhC02dgNMJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 3: Activation and Loss\n",
        "\n",
        "\n",
        "###3.1 **ACTIVATIONS:**<br>\n",
        "The activation functions for the RNN units (LSTM and GRU) are already integrated in the structure of this architecture. <br>\n",
        "For the hidden layer, the activation function I have choosen is **ReLU**.--> `'relu'`. <br>\n",
        "However, since the vectors have lots of zeros (and with ReLU many gradients will be zero), I decide to try also the **Sigmoid** --> `'sigmoid'`.<br> <br>\n",
        "\n",
        "###3.2 **LOSS FUNCTIONS:**<br>\n",
        "Since the target feature ('Ratings') is formed by **integer values**, the loss function for this task is the Sparse Categorical Cross Entropy (`sparse_categorical_crossentropy`).\n"
      ],
      "metadata": {
        "id": "yA_69-7QNP3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 4: Initializers, Regularizers, Normalizers\n",
        "\n",
        "### 4.1 **INITIALIZERS:**\n",
        "For the RNN layer and output layer I have used the **Glorot (Normal**) weight initialization. This initialization is very efficient for layers that have as activations sigmoid, tahn, softmax. --> `'glorot_normal'`<br>\n",
        "Instead for the hidden layer I have used the **He (Normal)** weight initialization that works more efficiently for layers that have ReLU as activation. --> `'he_normal`<br> <br>\n",
        "\n",
        "\n",
        "### 4.2 **REGULARIZERS**\n",
        "As regularizer in this model I have choosen to use the **Dropout**. With coding it is expressed as a new layer between the others layers. --> `Dropout(dropout_rate)`, where the dropout rate indicates how much percentage of the neurons will be removed temporarily. <br><br>\n",
        "\n",
        "\n",
        "### 4.3 **NORMALIZERS**\n",
        "In the project I mentioned the **Batch Normalization Layer** as a possible option as a normalizer. Since the Batch Normalization is useful when we deal with large networks architectures, in this case I decide to not use (our model is formed only by 3 layers)."
      ],
      "metadata": {
        "id": "GdDJMC_qN16V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the elements I described in point 2 (*output*), 3 (*activation, loss*), 4 (*regularizers, initializers, normalizers*) are structured in the following instruction code:\n",
        "- I defined a function `create_model` that contains the whole Sequential model (the 3 layers with corresponding hyperparameters) and the compile function that includes the loss function, the optimizer and the metric.\n",
        "- Then, I defined a **sample model** (`model`) with the function KerasClassifier of SkiKeras and I fit it on x,y train.\n",
        "- Finally, I try to show the accuracy on the training of this sample model."
      ],
      "metadata": {
        "id": "FgbaEJy_fAyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the model\n",
        "def create_model(RNN_type,\n",
        "                 nRNN,\n",
        "                 nhid,\n",
        "                 learning_rate= 0.001,\n",
        "                 hid_act='relu',\n",
        "                 out_act='softmax',\n",
        "                 dropout_rate=0.2,\n",
        "                 optimizer=SGD,\n",
        "                 epochs=10,\n",
        "                 batch_size=32):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(RNN_type(nRNN,\n",
        "                 input_shape=(150,1),\n",
        "                 kernel_initializer='glorot_normal'))\n",
        "  model.add(Dense(nhid,\n",
        "                  activation=hid_act,\n",
        "                  kernel_initializer='he_normal'))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(5,\n",
        "                  activation=out_act,\n",
        "                  kernel_initializer='glorot_normal'))\n",
        "  model.compile(loss='sparse_categorical_crossentropy',\n",
        "                optimizer=optimizer(learning_rate = learning_rate),\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Define the sample model\n",
        "model = KerasClassifier(model=create_model,\n",
        "                        RNN_type = LSTM,\n",
        "                        nRNN = 64,\n",
        "                        nhid = 64,\n",
        "                        epochs=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "yCN9pwizAuMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on the train\n",
        "model.fit(x_train, y_train, validation_split=0.2)\n",
        "pred = model.predict(x_train)\n",
        "\n",
        "# Show the Training accuracy\n",
        "print(f\"Training accuracy: {accuracy_score(y_train, pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTG_61UMHKcc",
        "outputId": "d1efe44e-2740-4398-a2f4-78dcaf63e429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 10s 266ms/step - loss: 1.6319 - accuracy: 0.1953 - val_loss: 1.6407 - val_accuracy: 0.2031\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 1.6350 - accuracy: 0.1836 - val_loss: 1.6399 - val_accuracy: 0.2031\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 43ms/step - loss: 1.6330 - accuracy: 0.2148 - val_loss: 1.6394 - val_accuracy: 0.2031\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 43ms/step - loss: 1.6312 - accuracy: 0.2070 - val_loss: 1.6385 - val_accuracy: 0.2031\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 57ms/step - loss: 1.6364 - accuracy: 0.2031 - val_loss: 1.6377 - val_accuracy: 0.2031\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 1.6304 - accuracy: 0.1992 - val_loss: 1.6369 - val_accuracy: 0.2031\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 1.6250 - accuracy: 0.2070 - val_loss: 1.6327 - val_accuracy: 0.2031\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.6241 - accuracy: 0.1914 - val_loss: 1.6320 - val_accuracy: 0.2031\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 1.6170 - accuracy: 0.1992 - val_loss: 1.6322 - val_accuracy: 0.2031\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 1.6318 - accuracy: 0.1875 - val_loss: 1.6320 - val_accuracy: 0.2031\n",
            "10/10 [==============================] - 1s 7ms/step\n",
            "Training accuracy: 0.203125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 5: Hyperparameters\n",
        "There are a lot of hyperparameters that I could train on the model. Since we have limited RAM, I decide to tune only some of them. I choose:\n",
        "- **RNN_type**: corresponds to the type of RNN architecture to use (LSTM or GRU).\n",
        "- **learning_rate**: corresponds to the learning rate of the optimizers. I choose 0.001 and 0.01 as possible values.\n",
        "- **hid_act**: corresponds to the activation of the hidden Dense Layer (ReLu or Sigmoid).\n",
        "- **nRNN**: number of units of the RNN part.\n",
        "- **nhid**: number of units of the hidden layer.\n",
        "- **optimizer**: corresponds to the type of optimizer (SGD or Adam).\n",
        "- **dropout_rate**: the percentage of neurons temporarily removed (0.3 or 0.2).\n",
        "- **batch_size**: how many inputs processed each time.\n",
        "\n",
        "Finally, I have applied a **GridSearch** to find the best model in performance with the best combination of the parameters."
      ],
      "metadata": {
        "id": "W52urK3NOAnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined the hyperparameters\n",
        "RNN_type = [LSTM, GRU]\n",
        "learning_rate = [0.01, 0.001]\n",
        "hid_act = ['relu', 'sigmoid']\n",
        "nRNN = [32, 64]\n",
        "nhid = [32, 64]\n",
        "optimizer = [SGD, Adam]\n",
        "dropout_rate = [0.2,0.3]\n",
        "batch_size = [32, 64]"
      ],
      "metadata": {
        "id": "7ZZwrlgBHVPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the parameters grid\n",
        "param_grid = dict(model__dropout_rate = dropout_rate,\n",
        "                  model__learning_rate = learning_rate,\n",
        "                  model__hid_act = hid_act,\n",
        "                  model__batch_size = batch_size,\n",
        "                  model__optimizer = optimizer,\n",
        "                  model__RNN_type = RNN_type,\n",
        "                  model__nhid= nhid,\n",
        "                  model__nRNN= nRNN)"
      ],
      "metadata": {
        "id": "QjW4spNzH_zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefined the model\n",
        "model = KerasClassifier(model=create_model,\n",
        "                        epochs = 10)"
      ],
      "metadata": {
        "id": "kqeQem1Ds8sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the Grid Search\n",
        "GS = GridSearchCV(estimator=model,\n",
        "                  param_grid=param_grid,\n",
        "                  n_jobs=-1,\n",
        "                  scoring='accuracy',\n",
        "                  refit=True,\n",
        "                  cv=3,\n",
        "                  verbose = 0)"
      ],
      "metadata": {
        "id": "nD6lBx8ItK5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PART 6: Evaluation\n",
        "\n",
        "In the evaluation part we fit the **GridSearch** and we see which model has obtained the best performance. <br>\n",
        "In our case the best model is composed by the following hyperparameters:\n",
        "- RNN_type = **LSTM**\n",
        "- batch_size = **32**\n",
        "- dropout_rate = **0.3**\n",
        "- hid_act = **sigmoid**\n",
        "- learning_rate = **0.001**\n",
        "- nRNN = **32**\n",
        "- nhid = **32**\n",
        "- optimizer = **Adam**\n",
        "\n",
        "This model has obtained an accuracy of 25% on the traning set. <br><br>\n",
        "\n",
        "Finally, we use this model to predict on the test and we obtain as accuracy 16% (worse than the traning)."
      ],
      "metadata": {
        "id": "8kzldjVuOE3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sol_model = GS.fit(x_train, y_train)\n",
        "print(f'\\tModel: Best score got by the best estimator: {sol_model.best_score_}')    # accuracy on the training\n",
        "print(f'\\tModel:Configuration for the best estimator/classifier: {sol_model.best_params_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmJ_QOeHtxKc",
        "outputId": "4d834b16-ea29-4b75-8dcd-e84770c48d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 3s 14ms/step - loss: 1.6309 - accuracy: 0.1656\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 1.6144 - accuracy: 0.2000\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.6005 - accuracy: 0.2156\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.5977 - accuracy: 0.2375\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.5890 - accuracy: 0.2344\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.5767 - accuracy: 0.2688\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.5879 - accuracy: 0.2750\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 1.5677 - accuracy: 0.2812\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.5742 - accuracy: 0.2625\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.5670 - accuracy: 0.2656\n",
            "\tModel: Best score got by the best estimator: 0.26250514312584494\n",
            "\tModel:Configuration for the best estimator/classifier: {'model__RNN_type': <class 'keras.layers.rnn.lstm.LSTM'>, 'model__batch_size': 64, 'model__dropout_rate': 0.3, 'model__hid_act': 'relu', 'model__learning_rate': 0.001, 'model__nRNN': 64, 'model__nhid': 64, 'model__optimizer': <class 'keras.optimizers.adam.Adam'>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on the test with the best estimator\n",
        "best_model = sol_model.best_estimator_\n",
        "pred_model = best_model.predict(x_test)\n",
        "model_acc = accuracy_score(y_test, pred_model)\n",
        "model_acc      # accuracy on the test"
      ],
      "metadata": {
        "id": "j9GUaD6W3TMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda14f3f-c97a-41b1-a151-fb77b2eba2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.225"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model: Mean Accuracy test:{np.mean(model_acc)}\")\n",
        "print(f\"Model: Standard Deviation test:{np.std(model_acc)}\")"
      ],
      "metadata": {
        "id": "DeBf19at3uA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ccdc48a-54b9-41ab-ccc5-5c157c533eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Mean Accuracy test:0.225\n",
            "Model: Standard Deviation test:0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL CONSIDERATIONS\n",
        "The performances of this model are not acceptables. But there are various reasons that we can claim to justify the results I obtained:\n",
        "- The dataset I use for the prediction is composed only by **400 rows**, that is a very small number compared to the original dataset that contains 42,000 rows (about 1000 times). The reason for which I have used a small the dataset regards the limitations of the RAM.\n",
        "- The dataset originally is very **imbalanced**. Most of the ratings correspond to 5 and this characteristics would influence too much the metrics (without balancing the dataset I would obtain 65% of accuracy on the test). For this reason I decide to undersampler the dataset but I use a technique that is not so efficient (RandomUnder Sampler) and I obtained these results.\n",
        "- Nowdays a lot of pretrained **embeddings** exist. Moreover, there are a lot of NLP techniques that generate very efficient embedding for texts and words. I choose to use one-hot encoding because it is the only one that we have seen during lectures.\n",
        "- Laslty, the **limitations** on RAM and computation units do not allow to construct great architectures."
      ],
      "metadata": {
        "id": "D-0HO8N2n_jX"
      }
    }
  ]
}